<!-- Observation 4: Scaling Is Failing -->
{% include observations/observation-detail.html
    id="observation-4"
    number="04"
    title="Scaling Is Failing"
    bg="bg-terminal-surface"
    problem="The AI industry has bet everything on a simple formula: more compute, more data, bigger models. $100B training runs are underway. Sam Altman expects to spend 'trillions' on infrastructure. Nuclear reactors are being restarted to power data centers. Yet smaller models now routinely outperform giants. Aya 8B beats BLOOM 176B with just 4.5% of the parameters. OpenAI, Google, and Anthropic are all struggling to build more advanced AI. The relationship between compute and performance is increasingly strained. Academia has been priced out entirely. We've reoriented our entire field around 'bigger is better.' Is it?"
    quote="We can now stray from the beaten path of boring, predictable gains from throwing compute at the problem."
    quote_source="<a href='https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5877662' target='_blank' class='text-terminal-green hover:underline'>Sara Hooker, On the Slow Death of Scaling (2025)</a>"
    why_matters="When scaling is the only path, only those with billions can participate. Research narrows. Innovation follows capital. Universities become irrelevant. But if scaling is failing, the billions being poured into data centers may be a colossal misallocation of resources, while more efficient approaches go unexplored."
    ohgod_solution="OHGOD!! What if the path forward isn't more compute, but smarter compute? What if a consumer GPU and 24 hours is enough? The scaling paradigm assumes resources are the constraint. We assume architecture and efficiency are. DeepSeek showed efficiency can beat scale. We aim to prove a single hobbyist GPU can too."
%}
