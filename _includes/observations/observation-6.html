<!-- Observation 6: The Architecture Barely Changes -->
{% include observations/observation-detail.html
    id="observation-6"
    number="06"
    title="The Architecture Barely Changes"
    bg="bg-terminal-surface"
    problem="The Transformer was invented in 2017. Eight years later, it still dominates, despite known limitations and promising alternatives. LLaMA-3 made no major architectural changes. Innovation requires experimentation, and experimentation requires affordable compute."
    quote="Despite many small and creative innovations since the original transformer architecture, there have not been any significant 'breakthrough' discoveries that have led to much better leaderboard results."
    quote_source="<a href='https://arxiv.org/abs/2408.00386v1' target='_blank' class='text-terminal-green hover:underline'>arXiv Survey (2024)</a>"
    why_matters="If the Transformer isn't optimal (and evidence suggests it may not be), then we're investing billions scaling a suboptimal design. Mamba offers 5x throughput. RWKV proves RNNs can compete. Griffin matches Llama-2 on 6x fewer tokens."
    ohgod_solution="OHGOD!! What if there's a better architecture waiting to be found? If you can train something in a day on a consumer GPU with your own evaluation, maybe you'll find it."
%}
